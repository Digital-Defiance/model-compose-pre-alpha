{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5f6f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of spain is \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 512/512 [00:46<00:00, 10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      " the capital of the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of  the city of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from GPT2.model import (GPT2LMHeadModel)\n",
    "from GPT2.utils import load_weight\n",
    "from GPT2.config import GPT2Config\n",
    "from GPT2.sample import sample_sequence\n",
    "from GPT2.encoder import get_encoder\n",
    "\n",
    "\n",
    "\n",
    "## INITIALISATION\n",
    "\n",
    "class Args(BaseModel):\n",
    "    text: str\n",
    "    quiet: Optional[bool] = False\n",
    "    nsamples: Optional[int] = 1\n",
    "    unconditional: Optional[bool] = False\n",
    "    batch_size: Optional[int] = 1\n",
    "    length: Optional[int] = -1\n",
    "    temperature: Optional[float] = 0.7\n",
    "    top_k: Optional[int] = 1\n",
    "\n",
    "args = Args(text=\"The capital of spain is \")\n",
    "        \n",
    "state_dict = torch.load(\n",
    "    'checkpoints/gpt2-pytorch_model.bin',\n",
    "    map_location='cpu' if not torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "assert args.nsamples % args.batch_size == 0\n",
    "\n",
    "seed = random.randint(0, 2147483647)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "enc = get_encoder()\n",
    "config = GPT2Config()\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "\n",
    "\n",
    "model = load_weight(model, state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "if args.length == -1:\n",
    "    args.length = config.n_ctx // 2\n",
    "elif args.length > config.n_ctx:\n",
    "    raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n",
    "\n",
    "print(args.text)\n",
    "context_tokens = enc.encode(args.text)\n",
    "generated = 0\n",
    "for _ in range(args.nsamples // args.batch_size):\n",
    "    out = sample_sequence(\n",
    "        model=model, \n",
    "        length=args.length,\n",
    "        context=context_tokens  if not  args.unconditional else None,\n",
    "        start_token=enc.encoder['<|endoftext|>'] if args.unconditional else None,\n",
    "        batch_size=args.batch_size,\n",
    "        temperature=args.temperature, top_k=args.top_k, device=device\n",
    "    )\n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    for i in range(args.batch_size):\n",
    "        generated += 1\n",
    "        text = enc.decode(out[i])\n",
    "        if args.quiet is False:\n",
    "            print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "print(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from GPT2.model import (GPT2LMHeadModel)\n",
    "from GPT2.utils import load_weight\n",
    "from GPT2.config import GPT2Config\n",
    "from GPT2.sample import sample_sequence\n",
    "from GPT2.encoder import get_encoder\n",
    "\n",
    "\n",
    "\n",
    "## INITIALISATION\n",
    "\n",
    "class Args(BaseModel):\n",
    "    text: str\n",
    "    quiet: Optional[bool] = False\n",
    "    nsamples: Optional[int] = 1\n",
    "    unconditional: Optional[bool] = False\n",
    "    batch_size: Optional[int] = 1\n",
    "    length: Optional[int] = -1\n",
    "    temperature: Optional[float] = 0.7\n",
    "    top_k: Optional[int] = 1\n",
    "\n",
    "args = Args(text=\"The capital of spain is \")\n",
    "        \n",
    "state_dict = torch.load(\n",
    "    'checkpoints/gpt2-pytorch_model.bin',\n",
    "    map_location='cpu' if not torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "assert args.nsamples % args.batch_size == 0\n",
    "\n",
    "seed = random.randint(0, 2147483647)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "enc = get_encoder()\n",
    "config = GPT2Config()\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "\n",
    "\n",
    "model = load_weight(model, state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "if args.length == -1:\n",
    "    args.length = config.n_ctx // 2\n",
    "elif args.length > config.n_ctx:\n",
    "    raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n",
    "\n",
    "print(args.text)\n",
    "context_tokens = enc.encode(args.text)\n",
    "generated = 0\n",
    "for _ in range(args.nsamples // args.batch_size):\n",
    "    out = sample_sequence(\n",
    "        model=model, \n",
    "        length=args.length,\n",
    "        context=context_tokens  if not  args.unconditional else None,\n",
    "        start_token=enc.encoder['<|endoftext|>'] if args.unconditional else None,\n",
    "        batch_size=args.batch_size,\n",
    "        temperature=args.temperature, top_k=args.top_k, device=device\n",
    "    )\n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    for i in range(args.batch_size):\n",
    "        generated += 1\n",
    "        text = enc.decode(out[i])\n",
    "        if args.quiet is False:\n",
    "            print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "print(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05625fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.65.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c37f38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_metadata',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'move_to_end',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ce936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
